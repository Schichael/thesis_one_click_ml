{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Union, Dict, Tuple, Sequence, Any\n",
    "import pandas as pd\n",
    "import abc\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from pycelonis.celonis_api.pql.pql import PQL, PQLColumn, PQLFilter\n",
    "from typing import Optional\n",
    "from pycelonis.celonis_api.process_analytics.analysis import Analysis\n",
    "import json\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class MajorAttribute(Enum):\n",
    "    ACTIVITY = \"Activity\"\n",
    "    CASE = \"Case\"\n",
    "    ordering = [ACTIVITY, CASE]\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.value <= other.value\n",
    "\n",
    "\n",
    "class MinorAttribute(abc.ABC):\n",
    "    def __init__(self, attribute_name: str, major_attribute: MajorAttribute):\n",
    "        self.attribute_name = attribute_name\n",
    "        self.major_attribute = major_attribute\n",
    "\n",
    "\n",
    "class CaseDurationMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Case duration\"\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class WorkInProgressMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Work in Progress\"\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class EventCountMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Event count\"\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class ReworkOccurenceMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Rework\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class ActivityOccurenceMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Activity occurence\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class EndActivityMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"End activity\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class StartActivityMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Start activity\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class ActivityTableColumnMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Activity table column\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "class CaseTableColumnMinorAttribute(MinorAttribute):\n",
    "    def __init__(self):\n",
    "        attribute_name = \"Case table column\"\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        super().__init__(attribute_name, major_attribute)\n",
    "\n",
    "\n",
    "class AttributeDataType(Enum):\n",
    "    NUMERICAL = \"numerical\"\n",
    "    CATEGORICAL = \"categorical\"\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.value <= other.value\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class Attribute:\n",
    "    major_attribute_type: MajorAttribute\n",
    "    minor_attribute_type: MinorAttribute\n",
    "    attribute_data_type: AttributeDataType\n",
    "    df_attribute_name: str\n",
    "    display_name: str\n",
    "    query: str\n",
    "    correlation: Optional[float] = 0.\n",
    "    p_val: Optional[float] = 1.\n",
    "    unit: Optional[str] = \"\"\n",
    "    column_name: str = None  # for generic column attributes\n",
    "    label_influence: Optional[float] = None  # for categorical attributes only\n",
    "    cases_with_attribute: Optional[int] = None  # for categorical attributes only\n",
    "\n",
    "\n",
    "class EmptyTable:\n",
    "    def __init__(self):\n",
    "        self.columns = []\n",
    "\n",
    "    def __bool__(self):\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_aggregation_display_name(agg):\n",
    "    if agg == \"MIN\":\n",
    "        return \"minimum\"\n",
    "    elif agg == \"MAX\":\n",
    "        return \"maximum\"\n",
    "    elif agg == \"AVG\":\n",
    "        return \"mean\"\n",
    "    elif agg == \"MEDIAN\":\n",
    "        return \"median\"\n",
    "    elif agg == \"FIRST\":\n",
    "        return \"first\"\n",
    "    elif agg == \"LAST\":\n",
    "        return \"last\"\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, datamodel, aggregations_dyn_cat=None, aggregations_dyn_num=None, gap=1, min_prefixes=1,\n",
    "            max_prefixes=20, min_occurrences=10, chunksize=10000, ):\n",
    "\n",
    "        self.dm = datamodel\n",
    "        if aggregations_dyn_cat is None:\n",
    "            self.aggregations_dyn_cat = [\"last\", \"sum\"]\n",
    "        else:\n",
    "            self.aggregations_dyn_cat = aggregations_dyn_cat\n",
    "\n",
    "        if aggregations_dyn_num is None:\n",
    "            self.aggregations_dyn_num = [\"last\", \"sum\"]\n",
    "        else:\n",
    "            self.aggregations_dyn_num = aggregations_dyn_num\n",
    "\n",
    "        self.categorical_types = [\"STRING\", \"BOOLEAN\"]\n",
    "        self.numerical_types = [\"INTEGER\", \"FLOAT\"]\n",
    "        self.gap = gap\n",
    "        self.min_prefixes = min_prefixes\n",
    "        self.max_prefixes = max_prefixes\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.chunksize = chunksize\n",
    "        # self.activity_df = None\n",
    "        # self.case_df = None\n",
    "        self.activity_table_name = None\n",
    "        self.case_table_name = None\n",
    "        self.activity_table = None\n",
    "        self.case_table = None\n",
    "        self.case_case_key = None\n",
    "        self.activity_case_key = None\n",
    "        self.prefixes_case_key = \"caseid_prefixes\"\n",
    "        self.activity_col = None\n",
    "        self.eventtime_col = None\n",
    "        self.sort_col = None\n",
    "        self.eventtime_col = None\n",
    "        self.static_numerical_cols = []\n",
    "        self.static_categorical_cols = []\n",
    "        self.dynamic_numerical_cols = []\n",
    "        self.dynamic_categorical_cols = []\n",
    "        self.static_categorical_values = {}\n",
    "        self.dynamic_categorical_values = {}\n",
    "        self.label_col = None\n",
    "        self.activity_start = \"Activity_START\"\n",
    "        self.activity_end = \"Activity_END\"\n",
    "        self.config_file_name = None\n",
    "        self.attributes = []\n",
    "        self.attributes_dict = {}\n",
    "        self.label = None\n",
    "        self.label_dict = {}\n",
    "        self._init_datamodel(self.dm)\n",
    "\n",
    "    def _init_datamodel(self, dm):\n",
    "        \"\"\"Initialize datamodel parameters\n",
    "\n",
    "        :param dm: input Datamodel\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get activity and case table IDs\n",
    "        activity_table_id = dm.data[\"processConfigurations\"][0][\"activityTableId\"]\n",
    "        case_table_id = dm.data[\"processConfigurations\"][0][\"caseTableId\"]\n",
    "        self.activity_table = dm.tables.find(activity_table_id)\n",
    "        self.eventtime_col = dm.data[\"processConfigurations\"][0][\"timestampColumn\"]\n",
    "        self.sort_col = dm.data[\"processConfigurations\"][0][\"sortingColumn\"]\n",
    "        self.activity_col = dm.data[\"processConfigurations\"][0][\"activityColumn\"]\n",
    "        self.activity_table_name = self.activity_table.name\n",
    "\n",
    "        if case_table_id:\n",
    "            self.case_table = dm.tables.find(case_table_id)\n",
    "\n",
    "            foreign_key_case_id = next((item for item in dm.data[\"foreignKeys\"] if\n",
    "            item[\"sourceTableId\"] == case_table_id and item[\"targetTableId\"] == activity_table_id), None, )\n",
    "\n",
    "            self.activity_case_key = foreign_key_case_id[\"columns\"][0][\"targetColumnName\"]\n",
    "\n",
    "            self.case_case_key = foreign_key_case_id[\"columns\"][0][\"sourceColumnName\"]\n",
    "            self.case_table_name = self.case_table.name\n",
    "            self._set_dynamic_features_PQL()\n",
    "            self._set_static_features_PQL()\n",
    "        else:\n",
    "            self.case_table = EmptyTable()\n",
    "            self.case_case_key = ''\n",
    "            self.case_table_name = ''\n",
    "            self.activity_case_key = dm.data[\"processConfigurations\"][0]['caseIdColumn']\n",
    "            self._set_dynamic_features_PQL()  # self.activity_df = activity_table.get_data_frame(chunksize=self.chunksize)  # self.case_df = case_table.get_data_frame(chunksize=chunksize)\n",
    "\n",
    "    def _set_static_features_PQL(self):\n",
    "        for attribute in self.case_table.columns:\n",
    "            if attribute['type'] in self.categorical_types and attribute['name'] not in [self.case_case_key,\n",
    "                                                                                         self.sort_col]:\n",
    "                self.static_categorical_cols.append(attribute['name'])\n",
    "            elif attribute['type'] in self.numerical_types and attribute['name'] not in [self.case_case_key,\n",
    "                                                                                         self.sort_col,\n",
    "                                                                                         ]:\n",
    "                self.static_numerical_cols.append(attribute['name'])\n",
    "\n",
    "    def _set_dynamic_features_PQL(self):\n",
    "        for attribute in self.activity_table.columns:\n",
    "            if attribute['type'] in self.categorical_types and attribute['name'] not in [self.activity_case_key,\n",
    "                                                                                         self.sort_col,\n",
    "                                                                                         self.activity_col]:\n",
    "                self.dynamic_categorical_cols.append(attribute['name'])\n",
    "            elif attribute['type'] in self.numerical_types and attribute['name'] not in [self.activity_case_key,\n",
    "                                                                                         self.sort_col]:\n",
    "                self.dynamic_numerical_cols.append(attribute['name'])\n",
    "\n",
    "    def _adjust_string_values(self, l: List[str]):\n",
    "        list_name = [el.replace('\"', '\\\\\"') for el in l]\n",
    "        list_val = [el.replace(\"'\", \"\\\\'\") for el in l]\n",
    "\n",
    "        return list_val, list_name\n",
    "\n",
    "    def compute_metrics(self, df, metrics=None):\n",
    "        if metrics is None:\n",
    "            metrics = ['influence', 'case_count', 'correlation']\n",
    "        for attr in self.attributes:\n",
    "            if attr.attribute_data_type != AttributeDataType.NUMERICAL:\n",
    "\n",
    "                if 'influence' in metrics:\n",
    "                    label_val_0 = df[df[attr.df_attribute_name] == 0][self.label.df_attribute_name].mean()\n",
    "                    label_val_1 = df[df[attr.df_attribute_name] == 1][self.label.df_attribute_name].mean()\n",
    "                    attr.label_influence = label_val_1 - label_val_0\n",
    "                if 'case_count' in metrics:\n",
    "                    attr.cases_with_attribute = len(df[df[attr.df_attribute_name] == 1].index)\n",
    "            if 'correlation' in metrics:\n",
    "                label_series = df[self.label.df_attribute_name]\n",
    "                attribute_df = pd.DataFrame(df[attr.df_attribute_name])\n",
    "                correlations = attribute_df.corrwith(label_series)\n",
    "                attr.correlation = correlations[attr.df_attribute_name]\n",
    "\n",
    "    def one_hot_encoding_PQL(self, table: str, case_id, attributes, major_attribute: MajorAttribute, minor_attribute,\n",
    "                             min_vals=1, suffix: str = \"\", prefix: str = \"\"):\n",
    "        if suffix != \"\":\n",
    "            suffix = \" \" + suffix\n",
    "        if prefix != \"\":\n",
    "            prefix = \" \" + prefix\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        for attribute in attributes:\n",
    "            query_unique = PQL()\n",
    "            query_unique.add(PQLColumn(name=\"values\", query=\"DISTINCT(\\\"\" + table + \"\\\".\\\"\" + attribute + \"\\\")\"))\n",
    "            query_unique.add(PQLColumn(name=\"count\", query=\"COUNT_TABLE(\\\"\" + self.case_table_name + \"\\\")\"))\n",
    "\n",
    "            df_unique_vals = self.dm.get_data_frame(query_unique)\n",
    "            # remove too few counts\n",
    "            df_unique_vals = df_unique_vals[df_unique_vals['count'] >= min_vals]\n",
    "            unique_values = list(df_unique_vals[\"values\"])\n",
    "            # Remove None values\n",
    "            unique_values = [x for x in unique_values if x is not None]\n",
    "\n",
    "            # Add escaping characters\n",
    "            unique_vals_val, unique_vals_name = self._adjust_string_values(unique_values)\n",
    "\n",
    "            if minor_attribute in [ActivityTableColumnMinorAttribute, CaseTableColumnMinorAttribute]:\n",
    "                column_name = attribute\n",
    "            else:\n",
    "                column_name = None\n",
    "\n",
    "            for val_val, val_name in zip(unique_vals_val, unique_vals_name):\n",
    "                df_attr_name = prefix + table + \"_\" + attribute + \" = \" + val_name + suffix\n",
    "                display_name = prefix + table + \".\" + attribute + \" = \" + val_name + suffix\n",
    "\n",
    "                major_attribute_type = major_attribute\n",
    "                minor_attribute_type = minor_attribute\n",
    "\n",
    "                query_val = \"SUM(CASE WHEN \\\"\" + table + \"\\\".\\\"\" + attribute + \"\\\" = '\" + val_val + \"' THEN 1 ELSE 0 \" \\\n",
    "                                                                                                    \"END)\"\n",
    "                attr_obj = Attribute(major_attribute_type, minor_attribute_type, AttributeDataType.CATEGORICAL,\n",
    "                                     df_attr_name, display_name, query_val, column_name=column_name)\n",
    "                self.attributes.append(attr_obj)\n",
    "                self.attributes_dict[df_attr_name] = attr_obj\n",
    "\n",
    "                query.add(PQLColumn(name=df_attr_name, query=query_val))\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        return dataframe\n",
    "\n",
    "    def _aggregate_static_categorical_PQL(self, min_vals: int):\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        minor_attribute = CaseTableColumnMinorAttribute()\n",
    "        df_static_categorical = self.one_hot_encoding_PQL(self.case_table_name, self.case_case_key,\n",
    "                                                          self.static_categorical_cols, major_attribute,\n",
    "                                                          minor_attribute, min_vals=min_vals)\n",
    "        # df_static_categorical = self.one_hot_encoding_PQL(self.case_table_name, self.case_case_key, self.static_categorical_cols)\n",
    "        # Remove values with too few occurences per case key, can this be done in PQL directly???\n",
    "        # df_static_categorical = df_static_categorical.loc[:, (df_static_categorical[df_static_categorical.drop(\n",
    "        # \"caseid\",  axis=1) > 0].count( axis=0) >= min_vals) | (df_static_categorical.columns == \"caseid\")]\n",
    "        df_static_categorical = self._conv_dtypes_PQL(df_static_categorical, [\"object\"], \"category\")\n",
    "        return df_static_categorical\n",
    "\n",
    "    def _aggregate_static_numerical_PQL(self):\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        for attribute in self.static_numerical_cols:\n",
    "            df_attr_name = self.case_table_name + \"_\" + attribute\n",
    "            display_name = self.case_table_name + \".\" + attribute\n",
    "\n",
    "            query_attr = \"\\\"\" + self.case_table_name + \"\\\".\" + \"\\\"\" + attribute + \"\\\"\"\n",
    "\n",
    "            query.add(PQLColumn(name=df_attr_name, query=query_attr))\n",
    "            attr_obj = Attribute(MajorAttribute.ACTIVITY, CaseTableColumnMinorAttribute(), AttributeDataType.NUMERICAL,\n",
    "                                 df_attr_name, display_name, query_attr, column_name=attribute)\n",
    "            self.attributes.append(attr_obj)\n",
    "            self.attributes_dict[df_attr_name] = attr_obj\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        return dataframe\n",
    "\n",
    "    def _aggregate_dynamic_categorical_PQL(self, min_vals: int = 1):\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = ActivityTableColumnMinorAttribute()\n",
    "        df_dynamic_categorical = self.one_hot_encoding_PQL(self.activity_table_name, self.activity_case_key,\n",
    "                                                           self.dynamic_categorical_cols, major_attribute,\n",
    "                                                           minor_attribute, min_vals=min_vals)\n",
    "        # Remove values with too few occurences per case key, can this be done in PQL directly???\n",
    "        # df_dynamic_categorical = df_dynamic_categorical.loc[:, (df_dynamic_categorical[\n",
    "        #                                                            df_dynamic_categorical.drop(\"caseid\",\n",
    "        #                                                                                        axis=1) > 0].count(\n",
    "        #    axis=0) >= min_vals) | (df_dynamic_categorical.columns == \"caseid\")]\n",
    "        df_dynamic_categorical[df_dynamic_categorical.drop('caseid', axis=1).columns] = df_dynamic_categorical[\n",
    "            df_dynamic_categorical.drop('caseid', axis=1).columns].apply(lambda x: self._binarize(x, 0), axis=1)\n",
    "        df_dynamic_categorical = self._conv_dtypes_PQL(df_dynamic_categorical, [\"object\"], \"category\")\n",
    "        return df_dynamic_categorical\n",
    "\n",
    "    def _aggregate_dynamic_numerical_PQL(self, aggregations=None):\n",
    "        if aggregations is None:\n",
    "            aggregations = ['AVG']\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        for agg in aggregations:\n",
    "            for attribute in self.dynamic_numerical_cols:\n",
    "                df_attr_name = self.activity_table_name + \".\" + agg + \"_\" + attribute\n",
    "                display_name = self.activity_table_name + \".\" + attribute + \" (\" + get_aggregation_display_name(\n",
    "                    agg) + \")\"\n",
    "                query_att = agg + \"(\\\"\" + self.activity_table_name + \"\\\".\" + \"\\\"\" + attribute + \"\\\")\"\n",
    "                query.add(PQLColumn(name=df_attr_name, query=query_att))\n",
    "                attr_obj = Attribute(MajorAttribute.ACTIVITY, ActivityTableColumnMinorAttribute(),\n",
    "                                     AttributeDataType.NUMERICAL,\n",
    "                                     df_attr_name, display_name, query_att, column_name=attribute)\n",
    "                self.attributes.append(attr_obj)\n",
    "                self.attributes_dict[df_attr_name] = attr_obj\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def _aggregate_dynamic_categorical_prefixes_PQL(self, max_prefixes, min_vals):\n",
    "        # one-hot-encoding\n",
    "        dataframe = self.one_hot_encoding_PQL(self.activity_table_name, self.activity_case_key,\n",
    "                                              self.dynamic_categorical_cols)\n",
    "\n",
    "        # remove too rare values\n",
    "        dataframe_enough_vals = dataframe.loc[:,\n",
    "                                (dataframe[dataframe.drop(\"caseid\", axis=1) > 0].count(axis=0) >= min_vals) | (\n",
    "                                        dataframe.columns == \"caseid\")]\n",
    "        query = PQL()\n",
    "        query.add(\n",
    "            PQLColumn(name=\"caseid\", query=\"\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\"\"))\n",
    "        # Running total\n",
    "        is_empty = True\n",
    "        for attribute in self.dynamic_categorical_cols:\n",
    "            query_unique = PQL()\n",
    "            query_unique.add(\n",
    "                PQLColumn(name=\"values\", query=\"DISTINCT(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + attribute + \"\\\")\"))\n",
    "            unique_values = list(self.dm.get_data_frame(query_unique)[\"values\"])\n",
    "            # Remove None values\n",
    "            unique_values = [x for x in unique_values if x is not None]\n",
    "            if unique_values:\n",
    "                is_empty = False\n",
    "            # Add escaping characters\n",
    "            unique_vals_val, unique_vals_name = self._adjust_string_values(unique_values)\n",
    "            for val_val, val_name in zip(unique_vals_val, unique_vals_name):\n",
    "                col_name = \"@@@\" + self.activity_table_name + \"@@\" + attribute + \"@\" + val_name\n",
    "                if col_name in dataframe_enough_vals.columns:\n",
    "                    query.add(\n",
    "                        PQLColumn(name=\"@@@@\" + self.activity_table_name + \"@@@\" + attribute + \"@@LAST@\" + val_name,\n",
    "                                  query=\"CASE WHEN \\\"\" + self.activity_table_name + \"\\\".\\\"\" + attribute + \"\\\" = '\" + val_val + \"' THEN 1 ELSE 0 END\"))\n",
    "                    query.add(\n",
    "                        PQLColumn(name=\"@@@@\" + self.activity_table_name + \"@@@\" + attribute + \"@@SUM@\" + val_name,\n",
    "                                  query=\"RUNNING_SUM(CASE WHEN \\\"\" + self.activity_table_name + \"\\\".\\\"\" + attribute + \"\\\" = '\" + val_val + \"' THEN 1 ELSE 0 END, PARTITION BY( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\"))\"))\n",
    "        if is_empty:\n",
    "            return None\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        dataframe = self._conv_dtypes_PQL(dataframe, [\"object\"], \"category\")\n",
    "\n",
    "        df_prefixes = self.extract_prefixes_categorical_PQL(dataframe, \"caseid\", max_prefixes)\n",
    "\n",
    "        return df_prefixes\n",
    "\n",
    "    def _aggregate_dynamic_numerical_prefixes_PQL(self, max_prefixes):\n",
    "        if not self.dynamic_numerical_cols:\n",
    "            return None\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(PQLColumn(name=\"caseid\",\n",
    "                            query=\"DISTINCT( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\")\"))\n",
    "        for attribute in self.dynamic_numerical_cols:\n",
    "            query.add(PQLColumn(name=\"@@@\" + self.activity_table_name + \"@@mean@\" + attribute,\n",
    "                                query=\"RUNNING_SUM(\\\"\" + self.activity_table_name + \"\\\".\" + \"\\\"\" + attribute + \"\\\", PARTITION BY( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\"))\"))\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        df_prefixes = self.extract_prefixes_numerical_PQL(dataframe, \"caseid\", max_prefixes)\n",
    "\n",
    "        return df_prefixes\n",
    "\n",
    "    def extract_prefixes_numerical_PQL(self, df, case_key, max_prefixes):\n",
    "        # Currently only average supported\n",
    "        df = df.copy()\n",
    "        df[\"case_length\"] = df.groupby(case_key, observed=True)[case_key].transform(len)\n",
    "\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [self.prefixes_case_key, \"num_activities\"])\n",
    "        for i in range(max_prefixes):\n",
    "            g = df.groupby(case_key, observed=True)\n",
    "            tmp = df[g.cumcount() == i]\n",
    "            columns_numeric = tmp.columns.tolist()\n",
    "            columns_numeric = [el for el in columns_numeric if el not in [case_key, \"case_length\"]]\n",
    "            tmp[columns_numeric] = tmp[columns_numeric] / (i + 1)\n",
    "            tmp[self.prefixes_case_key] = tmp[case_key].apply(lambda x: f\"{x}_{i + 1}\")\n",
    "            tmp[\"num_activities\"] = i + 1\n",
    "            # Add 'num_prefixes' to self.static_numerical_cols\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        df_prefixes = df_prefixes.drop(\"case_length\", axis=1)\n",
    "        return df_prefixes\n",
    "\n",
    "    def extract_prefixes_categorical_PQL(self, df, case_key, max_prefixes):\n",
    "        df = df.copy()\n",
    "        df[\"case_length\"] = df.groupby(case_key, observed=True)[case_key].transform(len)\n",
    "\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [self.prefixes_case_key, \"num_activities\"])\n",
    "        for i in range(max_prefixes):\n",
    "            g = df.groupby(case_key, observed=True)\n",
    "            tmp = df[g.cumcount() == i]\n",
    "            tmp[self.prefixes_case_key] = tmp[case_key].apply(lambda x: f\"{x}_{i + 1}\")\n",
    "            tmp[\"num_activities\"] = i + 1\n",
    "            # Add 'num_prefixes' to self.static_numerical_cols\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        df_prefixes = df_prefixes.drop(\"case_length\", axis=1)\n",
    "        return df_prefixes\n",
    "\n",
    "    def _extract_prefixes_remaining_time_PQL(self, df, max_prefixes):\n",
    "        df[\"case_length\"] = df.groupby(\"caseid\", observed=True)[\"caseid\"].transform(len)\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [self.prefixes_case_key])\n",
    "        for i in range(max_prefixes):\n",
    "            g = df.groupby(\"caseid\", observed=True)\n",
    "            tmp = df[g.cumcount() == i]\n",
    "            tmp[self.prefixes_case_key] = tmp[\"caseid\"].apply(lambda x: f\"{x}_{i + 1}\")\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        df_prefixes = df_prefixes.drop(\"case_length\", axis=1)\n",
    "        return df_prefixes\n",
    "\n",
    "    def remaining_time_PQL(self, max_prefixes):\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(PQLColumn(name=\"caseid\",\n",
    "                            query=\"TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\")\"))\n",
    "        query.add(PQLColumn(name=\"@@source\",\n",
    "                            query=\"SOURCE(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\", ANY_OCCURRENCE [ ] TO LAST_OCCURRENCE [ ])\"))\n",
    "        query.add(PQLColumn(name=\"@@target\",\n",
    "                            query=\"TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\")\"))\n",
    "        # query.add(PQLColumn(name=\"timestamp\", query=\"\\\"\" + activity_table + \"\\\".\\\"\" + timestamp_key + \"\\\"\"))\n",
    "        query.add(PQLColumn(name=\"@@time_to_end\",\n",
    "                            query=\"SECONDS_BETWEEN(SOURCE(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\"), TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\"))\"))\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        dataframe = dataframe.drop(['@@source', '@@target'], axis=1)\n",
    "        dataframe = self._extract_prefixes_remaining_time_PQL(dataframe, max_prefixes)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def get_query_case_ids(self):\n",
    "        return PQLColumn(name=\"caseid\", query=\"\\\"\" + self.case_table_name + \"\\\".\\\"\" + self.case_case_key + \"\\\"\")\n",
    "\n",
    "    def one_hot_encode_special(self, min_vals, query_str, attribute_name, major_attribute: MajorAttribute,\n",
    "                               minor_attribute: MinorAttribute, attribute_data_type: AttributeDataType):\n",
    "        \"\"\" One hot encoding with a special query.\n",
    "        query is string with what comes within the DISTINCT() brackets in the frist query and then in the CASE WHEN in the second query\n",
    "\n",
    "\n",
    "        :param min_vals: minimum values\n",
    "        :param query_str: the query\n",
    "        :param attribute_name: the attribute name in \"attribute_name = value\"\n",
    "        :param major_attribute: the major attribute\n",
    "        :param minor_attribute: the minor attribute\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        query_unique = PQL()\n",
    "        query_unique.add(PQLColumn(name=\"values\", query=\"DISTINCT(\" + query_str + \")\"))\n",
    "        query_unique.add(PQLColumn(name=\"count\", query=\"COUNT_TABLE(\\\"\" + self.case_table_name + \"\\\")\"))\n",
    "\n",
    "        df_unique_vals = self.dm.get_data_frame(query_unique)\n",
    "        # remove too few counts\n",
    "        df_unique_vals = df_unique_vals[df_unique_vals['count'] >= min_vals]\n",
    "        unique_values = list(df_unique_vals[\"values\"])\n",
    "        # Remove None values\n",
    "        unique_values = [x for x in unique_values if x is not None]\n",
    "        # Add escaping characters\n",
    "        unique_vals_val, unique_vals_name = self._adjust_string_values(unique_values)\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        for val_val, val_name in zip(unique_vals_val, unique_vals_name):\n",
    "            df_attr_name = attribute_name + \" = \" + val_name\n",
    "            display_name = attribute_name + \" = \" + val_val\n",
    "            query_attr = \"SUM(CASE WHEN \" + query_str + \" = \" + \"'\" + val_val + \"' THEN 1 ELSE 0 END)\"\n",
    "            query.add(PQLColumn(name=df_attr_name, query=query_attr))\n",
    "            attr_obj = Attribute(major_attribute, minor_attribute, attribute_data_type, df_attr_name,\n",
    "                                 display_name, query_attr)\n",
    "            self.attributes.append(attr_obj)\n",
    "            self.attributes_dict[df_attr_name] = attr_obj\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        return dataframe\n",
    "\n",
    "    def start_activity_PQL(self, min_vals):\n",
    "        attribute_name = \"Start activity\"\n",
    "        attribute_data_type = AttributeDataType.CATEGORICAL\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = StartActivityMinorAttribute()\n",
    "        query_str = \"PU_FIRST(\\\"\" + self.case_table_name + \"\\\", \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\")\"\n",
    "        df = self.one_hot_encode_special(min_vals, query_str, attribute_name, major_attribute, minor_attribute, attribute_data_type)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def end_activity_PQL(self, min_vals):\n",
    "        attribute_name = \"End activity\"\n",
    "        attribute_data_type = AttributeDataType.CATEGORICAL\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = EndActivityMinorAttribute()\n",
    "        query_str = \"PU_LAST(\\\"\" + self.case_table_name + \"\\\", \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\")\"\n",
    "        df = self.one_hot_encode_special(min_vals, query_str, attribute_name, major_attribute, minor_attribute, attribute_data_type)\n",
    "        return df\n",
    "\n",
    "    def start_activity_time_PQL(self):\n",
    "\n",
    "        query_str = \"PU_FIRST(\\\"\" + self.case_table_name + \"\\\", \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\")\"\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        query.add(PQLColumn(name=\"Case start time\", query=query_str))\n",
    "        df = self.dm.get_data_frame(query)\n",
    "        return df\n",
    "\n",
    "    def end_activity_time_PQL(self):\n",
    "        query_str = \"PU_LAST(\\\"\" + self.case_table_name + \"\\\", \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\")\"\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        query.add(PQLColumn(name=\"Case end time\", query=query_str))\n",
    "        df = self.dm.get_data_frame(query)\n",
    "        return df\n",
    "\n",
    "    def _binarize(self, x, th=1):\n",
    "        \"\"\"\n",
    "        set all values larger than th to 1, else to 0\n",
    "        x: Series\n",
    "\n",
    "        \"\"\"\n",
    "        x[x > th] = 1\n",
    "        x[x <= th] = 0\n",
    "        return x\n",
    "\n",
    "    def binary_activity_occurence_PQL(self, min_vals):\n",
    "        suffix = \"(occurence)\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = ActivityOccurenceMinorAttribute()\n",
    "        df_activities = self.one_hot_encoding_PQL(self.activity_table_name, self.activity_case_key, [self.activity_col],\n",
    "                                                  major_attribute, minor_attribute, min_vals, suffix)\n",
    "        # Remove values with too few occurences per case key, can this be done in PQL directly???\n",
    "\n",
    "        df_activities[df_activities.drop('caseid', axis=1).columns] = df_activities[\n",
    "            df_activities.drop('caseid', axis=1).columns].apply(lambda x: self._binarize(x, 0), axis=1)\n",
    "        df_activities = self._conv_dtypes_PQL(df_activities, [\"object\"], \"category\")\n",
    "        return df_activities\n",
    "\n",
    "    def _remove_rare_or_too_many(self, df, min_vals=0, max_vals=np.inf):\n",
    "        \"\"\" Remove column for which there are too many or too few 0 entries. Also entries are removed from\n",
    "        self.attributes and self.attributes_dict\n",
    "\n",
    "        :param df:\n",
    "        :param min_vals:\n",
    "        :param max_vals:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        drop_list = [i for i in df.columns if\n",
    "                     (((df[i] != 0).sum() <= min_vals) or ((df[i] != 0).sum() >= max_vals)) and (i != \"caseid\")]\n",
    "        df = df.drop(drop_list, axis=1)\n",
    "\n",
    "        self.attributes = [i for i in self.attributes if i.df_attribute_name not in drop_list]\n",
    "        for k in drop_list:\n",
    "            self.attributes_dict.pop(k, None)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def binary_rework_PQL(self, min_vals):\n",
    "        suffix = \"(rework)\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = ReworkOccurenceMinorAttribute()\n",
    "\n",
    "        df_activities = self.one_hot_encoding_PQL(self.activity_table_name, self.activity_case_key, [self.activity_col],\n",
    "                                                  major_attribute, minor_attribute, min_vals, suffix)\n",
    "        # Remove values with too few occurences per case key, can this be done in PQL directly???\n",
    "        df_activities[df_activities.drop('caseid', axis=1).columns] = df_activities[\n",
    "            df_activities.drop('caseid', axis=1).columns].apply(lambda x: self._binarize(x, 1), axis=1)\n",
    "        # remove attributes with too few 1 values\n",
    "\n",
    "        df_activities = self._remove_rare_or_too_many(df_activities, min_vals)\n",
    "\n",
    "        df_activities = self._conv_dtypes_PQL(df_activities, [\"object\"], \"category\")\n",
    "        return df_activities\n",
    "\n",
    "    def num_events(self):\n",
    "        df_attr_name = \"Event count\"\n",
    "        display_name = \"Event count\"\n",
    "        major_attribute = MajorAttribute.ACTIVITY\n",
    "        minor_attribute = \"Event count\"\n",
    "\n",
    "        q_num_events = \"PU_COUNT(\\\"\" + self.case_table_name + \"\\\", \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\")\"\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "        query.add(PQLColumn(name='num_events', query=q_num_events))\n",
    "        attr_obj = Attribute(major_attribute, EventCountMinorAttribute(), AttributeDataType.NUMERICAL, df_attr_name,\n",
    "                             display_name,\n",
    "                             q_num_events)\n",
    "        self.attributes.append(attr_obj)\n",
    "        self.attributes_dict[df_attr_name] = attr_obj\n",
    "        df = self.dm.get_data_frame(query)\n",
    "        return df\n",
    "\n",
    "    def work_in_progress_PQL(self, aggregations=None):\n",
    "        if aggregations is None:\n",
    "            aggregations = ['MIN', 'MAX', 'AVG']\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(self.get_query_case_ids())\n",
    "\n",
    "        for agg in aggregations:\n",
    "            agg_display_name = get_aggregation_display_name(agg)\n",
    "            df_attr_name = \"Work in progress\" + \" (\" + agg_display_name + \")\"\n",
    "            display_name = \"Work in progress\" + \" (\" + agg_display_name + \")\"\n",
    "            major_attribute = MajorAttribute.CASE\n",
    "\n",
    "            q = \"PU_\" + agg + \" ( \\\"\" + self.case_table_name + \"\\\", RUNNING_SUM( CASE WHEN INDEX_ACTIVITY_ORDER ( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\" ) = 1 THEN 1 WHEN INDEX_ACTIVITY_ORDER_REVERSE ( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\" ) = 1 THEN -1 ELSE 0 END, ORDER BY ( \\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\" ) ) )\"\n",
    "            query.add(PQLColumn(name=df_attr_name, query=q))\n",
    "            attr_obj = Attribute(major_attribute, WorkInProgressMinorAttribute(), AttributeDataType.NUMERICAL,\n",
    "                                 df_attr_name,\n",
    "                                 display_name, q)\n",
    "            self.attributes.append(attr_obj)\n",
    "            self.attributes_dict[df_attr_name] = attr_obj\n",
    "        df = self.dm.get_data_frame(query)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def run_total_time_PQL(self, min_vals, time_aggregation=\"DAYS\"):\n",
    "        start_activity_time_df = self.start_activity_time_PQL()\n",
    "        end_activity_time_df = self.end_activity_time_PQL()\n",
    "        start_activity_df = self.start_activity_PQL(min_vals)\n",
    "        end_activity_df = self.end_activity_PQL(min_vals)\n",
    "        binary_activity_occurence_df = self.binary_activity_occurence_PQL(min_vals)\n",
    "        binary_rework_df = self.binary_rework_PQL(min_vals)\n",
    "        work_in_progress_df = self.work_in_progress_PQL(aggregations=['AVG'])\n",
    "\n",
    "        static_cat_df = self._aggregate_static_categorical_PQL(min_vals)\n",
    "        static_num_df = self._aggregate_static_numerical_PQL()\n",
    "        dyn_cat_df = self._aggregate_dynamic_categorical_PQL(min_vals)\n",
    "        dyn_num_df = self._aggregate_dynamic_numerical_PQL()\n",
    "        total_time_df = self.total_time_PQL(time_aggregation, is_label=True)\n",
    "        joined_df = self._join_dfs([start_activity_time_df, end_activity_time_df, start_activity_df, end_activity_df,\n",
    "                                    binary_activity_occurence_df, binary_rework_df, work_in_progress_df, static_cat_df,\n",
    "                                    static_num_df, dyn_cat_df, dyn_num_df, total_time_df], keys=['caseid'] * 12)\n",
    "        self.compute_metrics(joined_df)\n",
    "        return joined_df\n",
    "\n",
    "    def total_time_PQL(self, time_aggregation, is_label: bool = False):\n",
    "\n",
    "        df_attr_name = \"case duration\"\n",
    "        display_name = \"case duration\"\n",
    "        major_attribute = MajorAttribute.CASE\n",
    "        minor_attribute = CaseDurationMinorAttribute()\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(PQLColumn(name=\"caseid\", query=\"\\\"\" + self.case_table_name + \"\\\".\\\"\" + self.case_case_key + \"\\\"\"))\n",
    "        q_total_time = (\n",
    "                \"(CALC_THROUGHPUT(ALL_OCCURRENCE['Process Start'] TO ALL_OCCURRENCE['Process End'], REMAP_TIMESTAMPS(\\\"\" + self.activity_table_name + '\".\"' + self.eventtime_col + '\", ' + time_aggregation + \")))\")\n",
    "        attr_obj = Attribute(major_attribute, minor_attribute, AttributeDataType.NUMERICAL, df_attr_name,\n",
    "                             display_name=display_name,\n",
    "                             query=q_total_time, unit = time_aggregation.lower())\n",
    "        if is_label:\n",
    "            self.label = attr_obj\n",
    "            self.label_dict[df_attr_name] = attr_obj\n",
    "        else:\n",
    "            self.attributes.append(attr_obj)\n",
    "            self.attributes_dict[df_attr_name] = attr_obj\n",
    "        query.add(PQLColumn(q_total_time, 'case duration'))\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        return dataframe\n",
    "\n",
    "    def _extract_prefixes_past_time_PQL(self, df, max_prefixes):\n",
    "        df[\"case_length\"] = df.groupby(\"caseid\", observed=True)[\"caseid\"].transform(len)\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [self.prefixes_case_key])\n",
    "        for i in range(max_prefixes):\n",
    "            g = df.groupby(\"caseid\", observed=True)\n",
    "            tmp = df[g.cumcount() == i]\n",
    "            tmp[self.prefixes_case_key] = tmp[\"caseid\"].apply(lambda x: f\"{x}_{i + 2}\")\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        df_prefixes = df_prefixes.drop(\"case_length\", axis=1)\n",
    "        return df_prefixes\n",
    "\n",
    "    def past_time_PQL(self, max_prefixes):\n",
    "        query = PQL()\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(PQLColumn(name=\"caseid\",\n",
    "                            query=\"TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_case_key + \"\\\")\"))\n",
    "        query.add(PQLColumn(name=\"@@source\",\n",
    "                            query=\"SOURCE(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\", FIRST_OCCURRENCE [ ] TO ANY_OCCURRENCE [ ])\"))\n",
    "        query.add(PQLColumn(name=\"@@target\",\n",
    "                            query=\"TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.activity_col + \"\\\")\"))\n",
    "        # query.add(PQLColumn(name=\"timestamp\", query=\"\\\"\" + activity_table + \"\\\".\\\"\" + timestamp_key + \"\\\"\"))\n",
    "        query.add(PQLColumn(name=\"@@past_time\",\n",
    "                            query=\"SECONDS_BETWEEN(SOURCE(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\"), TARGET(\\\"\" + self.activity_table_name + \"\\\".\\\"\" + self.eventtime_col + \"\\\"))\"))\n",
    "        dataframe = self.dm.get_data_frame(query)\n",
    "        dataframe = dataframe.drop(['@@source', '@@target'], axis=1)\n",
    "        dataframe = self._extract_prefixes_past_time_PQL(dataframe, max_prefixes)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def time_from_case_start(analysis, activity_table, case_id, attribute, timestamp_key):\n",
    "        query = PQL()\n",
    "\n",
    "        query.add(PQLColumn(name=\"values\", query=\"DISTINCT(\\\"\" + activity_table + \"\\\".\\\"\" + attribute + \"\\\")\"))\n",
    "\n",
    "        unique_values = list(analysis.get_data_frame(query)[\"values\"])\n",
    "\n",
    "        query = PQL()\n",
    "        query.add(PQLColumn(name=\"caseid\", query=\"TARGET(\\\"\" + activity_table + \"\\\".\\\"\" + case_id + \"\\\")\"))\n",
    "\n",
    "        for val in unique_values:\n",
    "            query.add(PQLColumn(name=\"@@min_time_from_start_\" + attribute + \"_\" + val,\n",
    "                                query=\"MIN(CASE WHEN TARGET(\\\"\" + activity_table + \"\\\".\\\"\" + attribute + \"\\\") = '\" + val + \"' THEN SECONDS_BETWEEN(SOURCE(\\\"\" + activity_table + \"\\\".\\\"\" + timestamp_key + \"\\\", FIRST_OCCURRENCE [ ] TO ANY_OCCURRENCE [ ]), TARGET(\\\"\" + activity_table + \"\\\".\\\"\" + timestamp_key + \"\\\")) ELSE NULL END)\"))\n",
    "\n",
    "        dataframe = analysis.get_data_frame(query)\n",
    "        return dataframe\n",
    "\n",
    "    def run_remaining_time_PQL(self, min_vals, max_prefixes):\n",
    "        static_cat_df = self._aggregate_static_categorical_PQL(min_vals)\n",
    "        static_num_df = self._aggregate_static_numerical_PQL()\n",
    "        dyn_cat_df_prefixes = self._aggregate_dynamic_categorical_prefixes_PQL(max_prefixes, min_vals)\n",
    "        dyn_num_df_prefixes = self._aggregate_dynamic_numerical_prefixes_PQL(max_prefixes)\n",
    "        past_time_df = self.past_time_PQL(max_prefixes)\n",
    "        remaining_time_df = self.remaining_time_PQL(max_prefixes)\n",
    "        # join dfs\n",
    "        # first with prefixes\n",
    "\n",
    "        joined_df = self._join_dfs([dyn_cat_df_prefixes, dyn_num_df_prefixes, past_time_df, remaining_time_df],\n",
    "                                   keys=[self.prefixes_case_key] * 4)\n",
    "\n",
    "        joined_df = self._join_dfs([joined_df, static_cat_df, static_num_df], keys=[\"caseid\"] * 3)\n",
    "\n",
    "        joined_df[\"@@time_to_end\"] = joined_df[\"@@time_to_end\"].fillna(0)\n",
    "        joined_df[\"@@past_time\"] = joined_df[\"@@past_time\"].fillna(0)\n",
    "\n",
    "        return joined_df\n",
    "\n",
    "    def _join_dfs(self, dfs: List[pd.DataFrame], keys: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Perform a Left outer join on two DataFrame. Only the key of the first\n",
    "        DataFrame is kept\n",
    "\n",
    "        :param dfs: list of at least two DataFrames\n",
    "        :param keys: columns to join on. But be of same length as dfs\n",
    "        :return: joined DataFrame\n",
    "        \"\"\"\n",
    "        df_result = None\n",
    "        for i in range(0, len(dfs)):\n",
    "            if dfs[i] is not None:\n",
    "                df_result = dfs[i]\n",
    "                break\n",
    "\n",
    "        for i in range(1, len(dfs)):\n",
    "            if dfs[i] is None:\n",
    "                continue\n",
    "            # Rmove common columns from one of those\n",
    "            common_columns = np.intersect1d(df_result.columns, dfs[i].columns).tolist()\n",
    "            if keys[i] in common_columns:\n",
    "                common_columns.remove(keys[i])\n",
    "            dfs[i] = dfs[i].drop(common_columns, axis=1)\n",
    "            df_result = pd.merge(df_result, dfs[i], how='left', left_on=keys[0], right_on=keys[i])\n",
    "\n",
    "            # Drop right key if it's different from the left key\n",
    "            if keys[0] != keys[i]:\n",
    "                df_result.drop(keys[i], axis=1)\n",
    "        return df_result\n",
    "\n",
    "    def save_config(self):\n",
    "        pass\n",
    "\n",
    "    def _conv_dtypes_PQL(self, df: pd.DataFrame, src_dtypes: List[str], target_dtype: str) -> pd.DataFrame:\n",
    "        \"\"\"Convert columns of types src_dtypes to datatype target_dtype\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param src_dtypes: list of data types to convert\n",
    "        :param target_dtype: datatype to convert to\n",
    "        :return: DatFrame with changed dtypes\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[df.select_dtypes(src_dtypes).columns] = df.select_dtypes(src_dtypes).apply(lambda x: x.astype(target_dtype))\n",
    "        return df\n",
    "\n",
    "    def _gen_prefixes(self, df: pd.DataFrame, case_key: str, prefixes_case_key: str, min_prefixes: int = 1,\n",
    "            max_prefixes: int = 20, gap: int = 1, ) -> pd.DataFrame:\n",
    "        \"\"\"Generate Prefixes. Columns that are added are: 'num_prefixes',\n",
    "        <prefixes_case_key>\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: case key\n",
    "        :param prefixes_case_key: case key for prefixes\n",
    "        :param min_prefixes: minimum prefixes\n",
    "        :param max_prefixes: maximum prefixes\n",
    "        :param gap: gap between prefixes\n",
    "        :return: DataFrame with prefixes\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"case_length\"] = df.groupby(case_key, observed=True)[case_key].transform(len)\n",
    "\n",
    "        # Create new DataFrame with columns from input df and column 'num_activities'\n",
    "        # which has the number of prefixes.\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [prefixes_case_key, \"num_activities\"])\n",
    "        for num_prefixes in range(min_prefixes, max_prefixes + 1, gap):\n",
    "            tmp = (df[df[\"case_length\"] >= num_prefixes].groupby(case_key, observed=True).head(num_prefixes))\n",
    "            tmp[prefixes_case_key] = tmp[case_key].apply(lambda x: f\"{x}_{num_prefixes}\")\n",
    "            tmp[\"num_activities\"] = num_prefixes\n",
    "            # Add 'num_prefixes' to self.static_numerical_cols\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        self.static_numerical_cols.append(\"num_activities\")\n",
    "        return df_prefixes\n",
    "\n",
    "    def _remove_few_occurrences(self, df: pd.DataFrame, columns: List[str], min_occurrences: int = 20):\n",
    "        \"\"\"Set values that occur to few times to np.nan\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param columns: columns from which to remove values with few occurences\n",
    "        :param min_occurrences: minimum occurrences\n",
    "        :return: dataframe with removed few occurrences\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Remove few occurances\n",
    "        for col_name in columns:\n",
    "            counts = df[col_name].value_counts()\n",
    "            repl = counts[counts < min_occurrences].index.tolist()\n",
    "            df[col_name].fillna(np.nan, inplace=True)\n",
    "            df[col_name].replace(repl, np.nan, inplace=True)\n",
    "            df[col_name] = df[col_name].cat.remove_unused_categories()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_few_occurrences_inference(self, df: pd.DataFrame, values_dict: Dict[str, Sequence[str]]):\n",
    "        \"\"\" Remove the (categorical) values that are not in values_dict.\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param values_dict: dictionary that contains the values for the categorical columns.\n",
    "        :return: DataFrame with removed few occurences\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Remove occurences that are not in values_dict\n",
    "        for col, vals in values_dict.items():\n",
    "\n",
    "            if col in df.columns:\n",
    "                df.loc[~df[col].isin(vals), col] = np.nan\n",
    "                df[col] = df[col].cat.remove_unused_categories()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_missing_columns(self, df: pd.DataFrame, cols: Sequence[str]):\n",
    "        \"\"\" Add columns from col into the DataFrame if they are not in it yet.\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param cols: column names\n",
    "        :return: the DataFrame with the added columns\n",
    "        \"\"\"\n",
    "        for col in cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def _save_categorical_values(self, activity_df: Optional[pd.DataFrame], case_df: Optional[pd.DataFrame]):\n",
    "        \"\"\" Save the categorical values in the corresponding dicts.\n",
    "\n",
    "        :param activity_df: activits DataFrame\n",
    "        :param case_df: case DataFrame\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if activity_df is not None:\n",
    "            for col in self.dynamic_categorical_cols:\n",
    "                unique_vals = activity_df[col].dropna().unique()\n",
    "                self.dynamic_categorical_values[col] = unique_vals.tolist()\n",
    "\n",
    "        if case_df is not None:\n",
    "            for col in self.static_categorical_cols:\n",
    "                unique_vals = case_df[col].dropna().unique()\n",
    "                self.static_categorical_values[col] = unique_vals.tolist()\n",
    "\n",
    "    def _remove_datetime(self, df: pd.DataFrame, exclude: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Remove datetime64 columns excluding the column specified 'exclude'\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param exclude: datetime column to exclude from being removed\n",
    "        :return: DataFrame with removed datetime columns\n",
    "        \"\"\"\n",
    "        datetime_cols = df.select_dtypes(\"datetime\").columns.tolist()\n",
    "        if exclude is not None and exclude in datetime_cols:\n",
    "            datetime_cols.remove(exclude)\n",
    "\n",
    "        if datetime_cols:\n",
    "            df = df.copy()\n",
    "            df = df.drop(datetime_cols, axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _conv_dtypes(self, df: pd.DataFrame, src_dtypes: List[str], target_dtype: str) -> pd.DataFrame:\n",
    "        \"\"\"Convert columns of types src_dtypes to datatype target_dtype\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param src_dtypes: list of data types to convert\n",
    "        :param target_dtype: datatype to convert to\n",
    "        :return: DatFrame with changed dtypes\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[df.select_dtypes(src_dtypes).columns] = df.select_dtypes(src_dtypes).apply(lambda x: x.astype(target_dtype))\n",
    "        return df\n",
    "\n",
    "    def _aggregate_static_categorical(self, df: pd.DataFrame, case_key: str, columns: List[str]):\n",
    "        \"\"\"Basically one-hot encoding of static categorical features (from case table)\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: the case key column, usually the prefix-case-key columns\n",
    "        :param columns: the columns to which to apply the aggregations\n",
    "        :return: DataFrame with aggregations\n",
    "        \"\"\"\n",
    "        if not columns:\n",
    "            df_agg = pd.DataFrame(df.groupby(case_key, observed=True)[case_key].agg(\"last\"))\n",
    "            df_agg.index.name = None\n",
    "            return df_agg\n",
    "\n",
    "        df_relevant = df[[case_key] + columns]\n",
    "        df_dummies_rel = pd.get_dummies(df_relevant, prefix=columns, prefix_sep=\" = \", columns=columns, sparse=True)\n",
    "        df_dummies_rel = df_dummies_rel.groupby(case_key, observed=True).agg(\"last\").reset_index()\n",
    "        return df_dummies_rel\n",
    "\n",
    "    def _aggregate_static_numerical(self, df: pd.DataFrame, case_key: str, columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Return the static numerical features (from case table) from the input\n",
    "        DataFrame\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: the case key column, usually the prefix-case-key columns\n",
    "        :param columns: the columns to which to apply the aggregations\n",
    "        :return: DataFrame with aggregations\n",
    "        \"\"\"\n",
    "        if not columns:\n",
    "            df_agg = pd.DataFrame(df.groupby(case_key, observed=True)[case_key].agg(\"last\"))\n",
    "            df_agg.index.name = None\n",
    "            return df_agg\n",
    "\n",
    "        df_relevant = df[[case_key] + columns]\n",
    "        df_relevant = df_relevant.groupby(case_key, observed=True).agg(\"last\").reset_index()\n",
    "        return df_relevant\n",
    "\n",
    "    def _aggregate_dynamic_categorical(self, df: pd.DataFrame, case_key: str, columns: List[str],\n",
    "            aggregations: List[str] = [\"last\", \"sum\"], ) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate dynamic categorical columns. For this, one-hot-encoding is applied\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: the case key column, usually the prefix-case-key columns\n",
    "        :param columns: the columns to which to apply the aggregations\n",
    "        :param aggregations: list with the aggregations to apply. The following\n",
    "        aggregations are possible: 'last' and 'sum'\n",
    "        :return: DataFrame with aggregations\n",
    "        \"\"\"\n",
    "\n",
    "        if not columns:\n",
    "            df_agg = pd.DataFrame(df.groupby(case_key, observed=True)[case_key].agg(\"last\"))\n",
    "            df_agg.index.name = None\n",
    "            return df_agg\n",
    "\n",
    "        df_relevant = df[[case_key] + columns]\n",
    "\n",
    "        df_dummies_rel = pd.get_dummies(df_relevant, prefix=columns, prefix_sep=\" = \", columns=columns, sparse=True)\n",
    "        col_names = df_dummies_rel.columns\n",
    "        df_agg = df_dummies_rel.groupby(case_key, observed=True).agg(aggregations).reset_index()\n",
    "        df_agg.columns = [\"_\".join(col).strip() if col != (case_key, \"\") else col[0] for col in df_agg.columns.values]\n",
    "\n",
    "        return df_agg\n",
    "\n",
    "    def _aggregate_dynamic_numerical(self, df: pd.DataFrame, case_key: str, columns: List[str],\n",
    "            aggregations: Dict[str, Union[Callable, str]] = {\"last\": \"last\", \"sum\": np.sum, \"mean\": np.mean,\n",
    "                \"std\": np.std, \"min\": np.min, \"max\": np.max, }, ) -> pd.DataFrame:\n",
    "        \"\"\"Use aggregations on numerical columns\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: the case key column, usually the prefix-case-key columns\n",
    "        :param columns: the columns to which to apply the aggregations\n",
    "        :param aggregations: dict containing a string as key that will be used for\n",
    "        the column name and a function or\n",
    "        string as value that can be used with pandas' DataFrameGroupBy.agg function.\n",
    "        :return: DataFrame with aggregations\n",
    "        \"\"\"\n",
    "        if not columns:\n",
    "            df_agg = pd.DataFrame(df.groupby(case_key, observed=True)[case_key].agg(\"last\"))\n",
    "            df_agg.index.name = None\n",
    "            return df_agg\n",
    "\n",
    "        df_relevant = df[[case_key] + columns]\n",
    "        df_agg = df_relevant.groupby(case_key, observed=True).agg(aggregations).reset_index()\n",
    "        df_agg.columns = [\"_\".join(col).strip() if col != (case_key, \"\") else col[0] for col in df_agg.columns.values]\n",
    "\n",
    "        df_agg.fillna(0, inplace=True)\n",
    "        return df_agg\n",
    "\n",
    "    def _compute_past_time(self, df: pd.DataFrame, case_key: str, eventtime_col: str,\n",
    "            past_time_col: Optional[str] = None, ):\n",
    "        \"\"\"Compute the past time from the start of the case till the end of the case.\n",
    "        The time is returned in minutes\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param case_key: name of the case key column (usually the prefix case_key\n",
    "        column)\n",
    "        :param time_col: name of the eventtime column\n",
    "        :param past_time_col: name to use for the column that has the past time. If\n",
    "        None, it will be <eventtime_col> + \"_PAST[minutes]\"\n",
    "        :return: DataFrame with past time column\n",
    "        \"\"\"\n",
    "\n",
    "        df_past_time = ((df.groupby(case_key, observed=True)[eventtime_col].last() -\n",
    "                         df.groupby(case_key, observed=True)[eventtime_col].first()) / pd.Timedelta(\n",
    "            \"1 minute\")).reset_index()\n",
    "\n",
    "        past_time_col_name = (past_time_col if past_time_col is not None else eventtime_col + \"_PAST[minutes]\")\n",
    "        df_past_time.rename(columns={eventtime_col: past_time_col_name}, inplace=True)\n",
    "\n",
    "        return df_past_time\n",
    "\n",
    "    def _get_cols_by_type(self, df: pd.DataFrame, dtypes: List[str], exclude: Optional[List[str]] = None) -> List[str]:\n",
    "        \"\"\"Get the column names of a DataFrame by dtype\n",
    "\n",
    "        :param df: input DataFrame\n",
    "        :param dtypes: dtypes of column names\n",
    "        :param exclude: List of column names to exclude\n",
    "        :return: List with the colun names of the specified dtypes\n",
    "        \"\"\"\n",
    "        if exclude is None:\n",
    "            exclude = []\n",
    "        cols = df.select_dtypes(dtypes)\n",
    "        cols = list(set(cols) - set(exclude))\n",
    "        return cols\n",
    "\n",
    "    def _rename_col_names(self, activity_df: Optional[pd.DataFrame], case_df: Optional[pd.DataFrame]):\n",
    "        \"\"\"Adds prefix \"<activity table name>_\" to activity table columns and\n",
    "        \"<case table name>_\" to case table columns. Also renames the relevant member\n",
    "        variables.\n",
    "\n",
    "        :param activity_df: activity DataFrame\n",
    "        :param case_df: case DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Columns to not rename\n",
    "        cols_not_rename = [self.case_case_key, self.activity_case_key, self.eventtime_col, self.sort_col,\n",
    "            self.activity_col, ]\n",
    "        if activity_df is not None:\n",
    "            activity_df.columns = [self.activity_table_name + \"_\" + col if col not in cols_not_rename else col for col\n",
    "                in activity_df.columns.values]\n",
    "        if case_df is not None:\n",
    "            case_df.columns = [self.case_table_name + \"_\" + col if col not in cols_not_rename else col for col in\n",
    "                case_df.columns.values]\n",
    "\n",
    "        # self.case_case_key = self.case_table_name + \"_\" + self.case_case_key  # self.activity_case_key = self.activity_table_name + \"_\" + self.activity_case_key  # self.activity_col = self.activity_table_name + \"_\" + self.activity_col  # self.eventtime_col = self.activity_table_name + \"_\" + self.eventtime_col  # self.sort_col = self.activity_table_name + \"_\" + self.sort_col\n",
    "\n",
    "    def _gen_label_future_activity(self, activity_df: pd.DataFrame, prefixes_df: pd.DataFrame,\n",
    "            activity_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Generates the labels for the use case if an activity happens in the future.\n",
    "\n",
    "        :param activity_df: the preprocessed activities DataFrame\n",
    "        :param prefixes_df: the prefixes DataFrame\n",
    "        :param activity_name: the name of the activity to check if it happens in the\n",
    "        future\n",
    "        :return: DataFrame with columns containing self.prefixes_case_key and the\n",
    "        corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Set label column\n",
    "        self.label_col = \"label_future_\" + activity_name\n",
    "\n",
    "        label_activities_df = pd.DataFrame(activity_df[self.activity_case_key])\n",
    "\n",
    "        # get number of appearences of activity for each case in activity_df\n",
    "        col_name_appearances = activity_name + \"_appearances\"\n",
    "        label_activities_df.loc[activity_df[self.activity_col] == activity_name, col_name_appearances] = 1\n",
    "        label_activities_df.loc[activity_df[self.activity_col] != activity_name, col_name_appearances] = 0\n",
    "        label_activities_df = label_activities_df.groupby(self.activity_case_key, observed=True, as_index=False)[\n",
    "            col_name_appearances].sum()\n",
    "        # add prefix_case_key\n",
    "        label_activities_df = self._join_dfs(\n",
    "            [label_activities_df, prefixes_df[[self.activity_case_key, self.prefixes_case_key]], ],\n",
    "            [self.activity_case_key, self.activity_case_key], )\n",
    "        label_activities_df = label_activities_df.drop_duplicates()\n",
    "\n",
    "        # Accumulate number of appearences of activity for each prefic_case in\n",
    "        # activity_df\n",
    "        label_prefixes_df = prefixes_df[[self.activity_case_key, self.prefixes_case_key]]\n",
    "        label_prefixes_df.loc[prefixes_df[self.activity_col] == activity_name, col_name_appearances] = 1\n",
    "        label_prefixes_df.loc[prefixes_df[self.activity_col] != activity_name, col_name_appearances] = 0\n",
    "        label_prefixes_df = label_prefixes_df.groupby(self.prefixes_case_key, observed=True, as_index=False).agg(\n",
    "            {col_name_appearances: \"sum\", self.activity_case_key: \"first\"})\n",
    "\n",
    "        # Sort DataFrames by prefixes_case_key\n",
    "        label_activities_df = label_activities_df.sort_values(by=[self.prefixes_case_key])\n",
    "        label_prefixes_df = label_prefixes_df.sort_values(by=[self.prefixes_case_key])\n",
    "\n",
    "        # Get labels where col_name_appearances is smaller in prefixes df than in\n",
    "        # activity df\n",
    "        labels = (label_prefixes_df[col_name_appearances].reset_index(drop=True) < label_activities_df[\n",
    "            col_name_appearances].reset_index(drop=True)).astype(int)\n",
    "\n",
    "        labels_df = pd.DataFrame(label_prefixes_df[self.prefixes_case_key])\n",
    "        labels_df[self.label_col] = labels\n",
    "\n",
    "        return labels_df\n",
    "\n",
    "    def _gen_label_remaining_execution_time(self, activity_df: pd.DataFrame, prefixes_df: pd.DataFrame):\n",
    "        \"\"\"Generate lables for remaining execution time.\n",
    "\n",
    "        :param activity_df: the preprocessed activities DataFrame\n",
    "        :param prefixes_df: the prefixes DataFrame\n",
    "        :return: DataFrame with columns containing self.prefixes_case_key and the\n",
    "        corresponding labels\n",
    "        \"\"\"\n",
    "        self.label_col = \"label_remaining_time\"\n",
    "        total_eventtime_df = self._compute_past_time(activity_df, self.activity_case_key,\n",
    "            eventtime_col=self.eventtime_col, past_time_col=\"total_time\", )\n",
    "        # add prefix_case_key\n",
    "        total_eventtime_df = self._join_dfs(\n",
    "            [total_eventtime_df, prefixes_df[[self.activity_case_key, self.prefixes_case_key]], ],\n",
    "            [self.activity_case_key, self.activity_case_key], )\n",
    "        total_eventtime_df = total_eventtime_df.drop_duplicates()\n",
    "\n",
    "        past_eventtime_df = self._compute_past_time(prefixes_df, self.prefixes_case_key,\n",
    "            eventtime_col=self.eventtime_col, past_time_col=\"past_time\", )\n",
    "\n",
    "        # Sort DataFrames by prefixes_case_key\n",
    "        total_eventtime_df = total_eventtime_df.sort_values(by=[self.prefixes_case_key])\n",
    "        past_eventtime_df = past_eventtime_df.sort_values(by=[self.prefixes_case_key])\n",
    "\n",
    "        # compute the remaining case time\n",
    "        remaining_time = total_eventtime_df[\"total_time\"].reset_index(drop=True) - past_eventtime_df[\n",
    "            \"past_time\"].reset_index(drop=True)\n",
    "\n",
    "        labels_df = pd.DataFrame(past_eventtime_df[self.prefixes_case_key])\n",
    "        labels_df[self.label_col] = remaining_time\n",
    "\n",
    "        return labels_df\n",
    "\n",
    "    def _gen_label_total_time(self, df: pd.DataFrame, case_key: str) -> pd.DataFrame:\n",
    "        \"\"\" Gererate label for total trace time\n",
    "\n",
    "        :param df: imput DataFrame\n",
    "        :param case_key: the case key to group on\n",
    "        :return: DataFrame with the added label\n",
    "        \"\"\"\n",
    "        self.label_col = \"label_TOTAL_TIME\"\n",
    "\n",
    "        return self._compute_past_time(df, case_key, self.eventtime_col, self.label_col)\n",
    "\n",
    "    def _gen_label_transition_times(self, prefixes_df: pd.DataFrame, activity_out: Sequence[str],\n",
    "            activity_in: Sequence[str]) -> pd.DataFrame:\n",
    "        \"\"\"Generate lables for transition times (The time from the 2nd last activity\n",
    "        to the last activity).\n",
    "\n",
    "        :param prefixes_df: the prefixes DataFrame\n",
    "        :param activity_out: outgoing activity\n",
    "        :param activity_in: incoming_activity\n",
    "        :return: DataFrame with prefix_case_key and transition_time of last transition.\n",
    "        \"\"\"\n",
    "        min_size = prefixes_df.groupby(self.prefixes_case_key, observed=True).size().min()\n",
    "\n",
    "        # print(prefixes_df)\n",
    "        # def pr(g):\n",
    "        #    #if len(g.index) == 1:\n",
    "        #    print(g)\n",
    "        # prefixes_df.groupby(self.prefixes_case_key, as_index=False).apply(pr)\n",
    "        # print(min_size)\n",
    "        def trans_time(g):\n",
    "            if len(g.index) < 2:\n",
    "                return\n",
    "            else:\n",
    "                return (g[self.eventtime_col].iloc[-1] - g[self.eventtime_col].iloc[-2]) / pd.Timedelta(\"1 minute\")\n",
    "\n",
    "        # labels_df = prefixes_df.groupby(self.prefixes_case_key, as_index=False)[\n",
    "        #    self.eventtime_col\n",
    "        # ].agg(lambda x: (x.iloc[-1] - x.iloc[-2]) / pd.Timedelta(\"1 minute\"))\n",
    "        labels_df = prefixes_df.groupby(self.prefixes_case_key, as_index=False, observed=True)[self.eventtime_col].agg(\n",
    "            lambda x: (x.iloc[-1] - x.iloc[-2]) / pd.Timedelta(\"1 minute\"))\n",
    "        self.label_col = f\"Transition time({activity_out}; {activity_in})\"\n",
    "        labels_df = labels_df.rename(columns={self.eventtime_col: self.label_col})\n",
    "        return labels_df\n",
    "\n",
    "    def _gen_label_next_activity(self, prefixes_df: pd.DataFrame,\n",
    "            incoming_activities: Optional[Sequence[str]]) -> pd.DataFrame:\n",
    "        \"\"\"Generate labels for the next activities\n",
    "\n",
    "        :param prefixes_df: input DataFrame\n",
    "        :param incoming_activities: the incoming activities for which to create\n",
    "        labels. For all other incoming activities, one additional label is created.\n",
    "        :return: label DataFrame\n",
    "        \"\"\"\n",
    "        self.label_col = \"label_next_Activity\"\n",
    "        labels = pd.DataFrame()\n",
    "        labels[[self.prefixes_case_key, self.label_col]] = prefixes_df.groupby(self.prefixes_case_key, observed=True,\n",
    "            as_index=False).apply(lambda x: x[self.activity_col].iloc[-1])\n",
    "\n",
    "        # Rename incoming activities that are not in incoming_activities\n",
    "\n",
    "        if incoming_activities is not None:\n",
    "            labels.loc[~labels[self.label_col].isin(incoming_activities), self.label_col] = \"OTHER\"\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _remove_last_row(self, prefixes_df) -> pd.DataFrame:\n",
    "        \"\"\"Remove the last row of all case. This is needed for the decision points\n",
    "        use case.\n",
    "\n",
    "        :param prefixes_df: Prefixes DataFrame\n",
    "        :return: DataFrame with removed last rows\n",
    "        \"\"\"\n",
    "        prefixes_df = prefixes_df[\n",
    "            prefixes_df.groupby(self.prefixes_case_key, observed=True).cumcount(ascending=False) > 0]\n",
    "        return prefixes_df\n",
    "\n",
    "    def _adjust_eventtime_transition_times(self, prefixes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Set the eventtime of the last activity in a prefix-case to the same value\n",
    "        as the previous activity. This is done to not have the time of the last\n",
    "        transition as part of the past-time feature.\n",
    "\n",
    "        :param prefixes_df: the prefixes DataFrame\n",
    "        :return: DataFrame with adjusted eventtimes.\n",
    "        \"\"\"\n",
    "\n",
    "        def overwrite(group):\n",
    "            group[self.eventtime_col].iloc[-1] = group[self.eventtime_col].iloc[-2]\n",
    "            return group\n",
    "\n",
    "        prefixes_df = prefixes_df.groupby(self.activity_case_key, observed=True).apply(overwrite)\n",
    "        return prefixes_df\n",
    "\n",
    "    def _gen_prefixes_transitions(self, df: pd.DataFrame, activity_out: Optional[List[str]] = None,\n",
    "            activity_in: Optional[List[str]] = None, ) -> pd.DataFrame:\n",
    "        \"\"\"Generate prefixes for transition times. This is needed because a\n",
    "        transition can happen multiple times. The output DataFrame's prefix-cases\n",
    "        will always end with the activity_in activity.\n",
    "\n",
    "        :param df: activity DataFrame\n",
    "        :param activity_out: outgoing activity\n",
    "        :param activity_in: incoming activities. If None, all transitions from\n",
    "        activity_out are taken\n",
    "\n",
    "        return: prefix DataFrame\n",
    "        \"\"\"\n",
    "        if activity_in is None:\n",
    "            activity_in = df[self.activity_col].unique()\n",
    "        if activity_out is None:\n",
    "            activity_out = df[self.activity_col].unique()\n",
    "        # Identify the rows of the transitions (row of incoming activity)\n",
    "        mask_transitions = (\n",
    "                ((df[self.activity_col].isin(activity_out)).shift(1)) & (df[self.activity_col].isin(activity_in)) & (\n",
    "                    df[self.activity_case_key].shift(1) == df[self.activity_case_key]))\n",
    "\n",
    "        # To binary\n",
    "        df_mask = pd.DataFrame(df[self.activity_case_key])\n",
    "        df_mask[\"bin_transitions\"] = mask_transitions\n",
    "        # cumulate backwards\n",
    "        df_mask = (df_mask.iloc[::-1].groupby(self.activity_case_key, observed=True, as_index=False)[\n",
    "                       \"bin_transitions\"].cumsum()[::-1])\n",
    "        # Get the maximum number of these transitions (minus 1)\n",
    "        max_transitions = int(df_mask[\"bin_transitions\"].max())\n",
    "\n",
    "        df_prefixes = pd.DataFrame(columns=df.columns.tolist() + [self.prefixes_case_key, \"num_activities\"])\n",
    "\n",
    "        def num_activities(group):\n",
    "            g = group[self.activity_case_key].size\n",
    "            group[\"num_activities\"] = g\n",
    "            return group\n",
    "\n",
    "        self.static_numerical_cols.append(\"num_activities\")\n",
    "        for i in range(1, max_transitions + 1):\n",
    "            tmp = df[df_mask[\"bin_transitions\"] >= i]\n",
    "            tmp = tmp.groupby(self.activity_case_key, as_index=False, observed=True).apply(num_activities)\n",
    "            tmp[self.prefixes_case_key] = tmp[self.activity_case_key].apply(lambda x: f\"{x}_{i}\")\n",
    "            df_prefixes = pd.concat([df_prefixes, tmp], axis=0)\n",
    "        return df_prefixes\n",
    "\n",
    "    def _query_datamodel_transition(self, activity_out: Optional[List[str]], activity_in: Optional[List[str]] = None) -> \\\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Get activity and case table as DataFrames. Only the cases are queried that\n",
    "        have a transition from activity_1 to activity_2\n",
    "\n",
    "        :param activity_out: outgoing activity\n",
    "        :param activity_in: incoming activites. If None, all activities are chosen\n",
    "        as possible incoming activities.\n",
    "        return: activity and case DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        columns_activities = [item[\"name\"] for item in self.activity_table.columns]\n",
    "        columns_cases = [item[\"name\"] for item in self.case_table.columns]\n",
    "\n",
    "        # Queries for all columns\n",
    "        query_activities = PQL()\n",
    "        for col in columns_activities:\n",
    "            query_activities += PQLColumn('\"' + self.activity_table.name + '\"' + \".\" + '\"' + col + '\"', col)\n",
    "\n",
    "        query_cases = PQL()\n",
    "        for col in columns_cases:\n",
    "            query_cases += PQLColumn('\"' + self.case_table.name + '\"' + \".\" + '\"' + col + '\"', col)\n",
    "\n",
    "        # Add filter for transition\n",
    "        if activity_in is None:\n",
    "            term_activity_in = 'ANY'\n",
    "        else:\n",
    "            term_activity_in = (\"(\" + \", \".join(\"'\" + act + \"'\" for act in activity_in) + \")\")\n",
    "\n",
    "        if activity_out is None:\n",
    "            term_activity_out = 'ANY'\n",
    "        else:\n",
    "            term_activity_out = (\"(\" + \", \".join(\"'\" + act + \"'\" for act in activity_out) + \")\")\n",
    "        query_cases += PQLFilter(f\"PROCESS EQUALS {term_activity_out} TO {term_activity_in}\")\n",
    "        query_activities += PQLFilter(f\"PROCESS EQUALS {term_activity_out} TO {term_activity_in}\")\n",
    "\n",
    "        activity_df = self.dm.get_data_frame(query_activities)\n",
    "\n",
    "        if self.case_table:\n",
    "            case_df = self.dm.get_data_frame(query_cases)\n",
    "        else:\n",
    "            case_df = pd.DataFrame()\n",
    "        return activity_df, case_df\n",
    "\n",
    "    def _add_start_end_activities(self, activity_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add a Start and End activity to each case. The eventtime of the start\n",
    "        activity will be the same as the eventtime of the following activity minus\n",
    "        1ns. The eventtime of the end activity will be the same as the eventtime of\n",
    "        the previous activity plus 1ns.\n",
    "        \"\"\"\n",
    "\n",
    "        def gen_start_end_row(group):\n",
    "            df_new = pd.DataFrame({self.activity_case_key: [group.iloc[0][self.activity_case_key]] * 2,\n",
    "                self.activity_col: [self.activity_start, self.activity_end],\n",
    "                self.eventtime_col: [group.iloc[0][self.eventtime_col] - pd.Timedelta(1),\n",
    "                                     group.iloc[-1][self.eventtime_col] + pd.Timedelta(1), ], })\n",
    "            complete_df = pd.concat([pd.DataFrame(df_new.iloc[0]).T, group, pd.DataFrame(df_new.iloc[1]).T])\n",
    "            return complete_df\n",
    "\n",
    "        df_added = activity_df.groupby(self.activity_case_key, observed=True, as_index=False).apply(gen_start_end_row)\n",
    "        # df_added = df_added.droplevel(0).reset_index(drop=True)\n",
    "        return df_added\n",
    "\n",
    "    def _query_datamodel(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Get activity and case table as DataFrames. The whole tables are queried\n",
    "\n",
    "        return: activity and case DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        activity_df = self.activity_table.get_data_frame(chunksize=self.chunksize)\n",
    "        if self.case_table:\n",
    "            case_df = self.case_table.get_data_frame(chunksize=self.chunksize)\n",
    "        else:\n",
    "            case_df = pd.DataFrame()\n",
    "\n",
    "        return activity_df, case_df\n",
    "\n",
    "    def _query_datamodel_only_case(self) -> pd.DataFrame:\n",
    "        \"\"\"Get case table as DataFrames. The whole table is queried\n",
    "\n",
    "        return: case DataFrame\n",
    "        \"\"\"\n",
    "        case_df = self.case_table.get_data_frame(chunksize=self.chunksize)\n",
    "\n",
    "        return case_df\n",
    "\n",
    "    def _gen_labels_conforming(self, df: pd.DataFrame, case_key_col: str,\n",
    "                               case_keys_conforming: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\" Generate labels for conforming cases ('conforming' and 'not conforming')\n",
    "\n",
    "        :param case_df: input DataFrame\n",
    "        :param case_key_col: name of the case key column\n",
    "        :param case_key_conforming: are with the case_keys of the conforming cases\n",
    "        :return: the DataFrame with the case keys and labels\n",
    "        \"\"\"\n",
    "        self.label_col = 'label_conforming'\n",
    "\n",
    "        def gen_conforming(g):\n",
    "            if g[case_key_col].values[0] in case_keys_conforming:\n",
    "                g[self.label_col] = 1\n",
    "                return g[[case_key_col, self.label_col]]\n",
    "            else:\n",
    "                g[self.label_col] = 0\n",
    "                return g[[case_key_col, self.label_col]]\n",
    "\n",
    "        labels = df.groupby(case_key_col, as_index=False, observed=True).apply(gen_conforming)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _query_case_keys_conforming(self, analysis: Analysis, shared_url: str) -> np.ndarray:\n",
    "        \"\"\" Query the case keys of the cases that are conforming to the selection in the shared url.\n",
    "\n",
    "        :param analysis: the analysis from which to use the shared_url\n",
    "        :param shared_url: the shared url from the analysis\n",
    "        :return: array with the case key from the case table of the conforming cases\n",
    "        \"\"\"\n",
    "        pql_shared = analysis.process_shared_selection_url(shared_url)\n",
    "        query = PQL()\n",
    "        query += PQLColumn('\"' + self.case_table_name + '\"' + \".\" + '\"' + self.case_case_key + '\"', self.case_case_key)\n",
    "        query += pql_shared\n",
    "        dm = analysis.datamodel\n",
    "        df = self.dm.get_data_frame(query)\n",
    "        case_keys = df[self.case_case_key].values\n",
    "        return case_keys\n",
    "\n",
    "    def _remove_activity_status(self, activity_df: pd.DataFrame):\n",
    "        if 'lifecycle:transition' in activity_df.columns:\n",
    "\n",
    "            activity_df = activity_df[activity_df['lifecycle:transition'] == 'COMPLETE']\n",
    "            activity_df = activity_df.drop('lifecycle:transition', axis=1)\n",
    "            return activity_df\n",
    "        else:\n",
    "            return activity_df\n",
    "\n",
    "    def _set_latest_date(self):\n",
    "        query = PQL()\n",
    "        query += PQLColumn(\"MAX(\" + '\"' + self.activity_table_name + '\"' + \".\" + '\"' + self.eventtime_col + '\"' + \")\",\n",
    "                           'latest_date')\n",
    "        df = self.dm.get_data_frame(query)\n",
    "        self.latest_date = df['latest_date'][0]\n",
    "\n",
    "    def _processed_case_table_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Process case DataFrame for training data. This sets the\n",
    "        member variables self.static_numerical_cols and\n",
    "        self.static_categorical_cols.\n",
    "        It processes the tables in the following order:\n",
    "        1. Get DataFrame from the Database\n",
    "        2. Convert object dtypes to categorical\n",
    "        3. Remove datetime columns\n",
    "        4. Rename case columns to \"<casetable name>_<column_name>\"\n",
    "        5. Define static columns (the member variables)\n",
    "        6. Remove few occurrences(defined by self.min_occurrences) of categorical\n",
    "        columns except the activity column\n",
    "\n",
    "        :return: processed case DataFrame\n",
    "        \"\"\"\n",
    "        # Get DataFrame\n",
    "        case_df = self._query_datamodel_only_case()\n",
    "\n",
    "        # Convert object dtype to categorical\n",
    "        case_df = self._conv_dtypes(case_df, [\"object\"], \"category\")\n",
    "\n",
    "        # Remove datetime columns\n",
    "        case_df = self._remove_datetime(case_df)\n",
    "\n",
    "        # Rename case columns to \"<casetable name>_<column_name>\"\n",
    "        self._rename_col_names(None, case_df)\n",
    "\n",
    "        # Define static and dynamic columns\n",
    "        self.static_numerical_cols = self._get_cols_by_type(case_df, dtypes=[\"number\"], exclude=[self.case_case_key])\n",
    "        self.static_categorical_cols = self._get_cols_by_type(case_df, dtypes=[\"category\"],\n",
    "            exclude=[self.case_case_key])\n",
    "\n",
    "        # Remove few occurrences of categorical columns\n",
    "        dynamic_cat_cols_without_activity = filter(lambda x: x != self.activity_col, self.dynamic_categorical_cols)\n",
    "\n",
    "        case_df = self._remove_few_occurrences(case_df, self.static_categorical_cols, self.min_occurrences)\n",
    "\n",
    "        # Save the categorical values\n",
    "        self._save_categorical_values(None, case_df)\n",
    "\n",
    "        return case_df\n",
    "\n",
    "    def _processed_activity_case_tables_train(self, transition: Optional[Tuple[List[str], List[str]]] = None,\n",
    "            inference: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Process activity and case DataFrames for training data. This sets the\n",
    "        member variables self.static_numerical_cols,\n",
    "        self.static_categorical_cols, self.dynamic_numerical_cols,\n",
    "        and self.dynamic_categorical_cols.\n",
    "        It processes the tables in the following order:\n",
    "        1. Get DataFrames from the Database\n",
    "        2. Convert object dtypes to categorical\n",
    "        3. Remove datetime columns except the eventtime column in activity_df\n",
    "        4. Remove sorting column from activity table\n",
    "        5. Rename activity columns to \"<activitytable name>_<column_name>\" and case\n",
    "        columns to \"<casetable name>_<column_name>\"\n",
    "        6. Define static and dynamic columns (the member variables)\n",
    "        7. Remove few occurrences(defined by self.min_occurrences) of categorical\n",
    "        columns except the activity column\n",
    "\n",
    "        :param transition: Tuple with outgoing and incoming activity. Used if only data\n",
    "        with such a transition shall be queried.\n",
    "        :param inference: if True, the loaded data from the json config is used.\n",
    "        :return: processed activity and case DataFrames\n",
    "        \"\"\"\n",
    "        # Get DataFrames\n",
    "        if transition:\n",
    "            activity_df, case_df = self._query_datamodel_transition(transition[0], transition[1])\n",
    "        else:\n",
    "            activity_df, case_df = self._query_datamodel()\n",
    "\n",
    "        # Convert object dtype to categorical\n",
    "        activity_df = self._conv_dtypes(activity_df, [\"object\"], \"category\")\n",
    "        case_df = self._conv_dtypes(case_df, [\"object\"], \"category\")\n",
    "\n",
    "        # Remove all transitions that are not completed\n",
    "        activity_df = self._remove_activity_status(activity_df)\n",
    "\n",
    "        # Save latest eventtime date\n",
    "        self.latest_date = activity_df[self.eventtime_col].max()\n",
    "\n",
    "        # Remove datetime columns except the eventtime column in activity_df\n",
    "        activity_df = self._remove_datetime(activity_df, self.eventtime_col)\n",
    "        case_df = self._remove_datetime(case_df)\n",
    "        # Remove sorting column from activity table\n",
    "        if self.sort_col is not None:\n",
    "            activity_df.drop(self.sort_col, inplace=True, axis=1)\n",
    "\n",
    "        # Rename activity columns to \"<activitytable name>_<column_name>\"\n",
    "        # and case columns to \"<casetable name>_<column_name>\"\n",
    "        self._rename_col_names(activity_df, case_df)\n",
    "\n",
    "        # Define static and dynamic columns\n",
    "\n",
    "        self.static_numerical_cols = self._get_cols_by_type(case_df, dtypes=[\"number\"], exclude=[self.case_case_key])\n",
    "        self.dynamic_numerical_cols = self._get_cols_by_type(activity_df, dtypes=[\"number\"],\n",
    "            exclude=[self.activity_case_key])\n",
    "        self.static_categorical_cols = self._get_cols_by_type(case_df, dtypes=[\"category\"],\n",
    "            exclude=[self.case_case_key])\n",
    "\n",
    "        self.dynamic_categorical_cols = self._get_cols_by_type(activity_df, dtypes=[\"category\"],\n",
    "            exclude=[self.activity_case_key])\n",
    "\n",
    "        # Remove few occurrences of categorical columns except the activity column (if inference = False, else also for activity column)\n",
    "\n",
    "        if inference:\n",
    "            activity_df = self._remove_few_occurrences_inference(activity_df, self.dynamic_categorical_values)\n",
    "\n",
    "            case_df = self._remove_few_occurrences_inference(case_df, self.static_categorical_values)\n",
    "        else:\n",
    "            dynamic_cat_cols_without_activity = filter(lambda x: x != self.activity_col, self.dynamic_categorical_cols)\n",
    "            activity_df = self._remove_few_occurrences(activity_df, dynamic_cat_cols_without_activity,\n",
    "                self.min_occurrences)\n",
    "\n",
    "            case_df = self._remove_few_occurrences(case_df, self.static_categorical_cols, self.min_occurrences)\n",
    "\n",
    "        # Save the categorical values if inference == False\n",
    "        if not inference:\n",
    "            self._save_categorical_values(activity_df, case_df)\n",
    "\n",
    "        return activity_df, case_df\n",
    "\n",
    "    def _load_config(self, filename: str):\n",
    "        \"\"\" Load config from a json file\n",
    "        :param filename: jilename of the json file without extension.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(filename + '.json') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            self.final_columns = data['final_columns']\n",
    "            self.static_categorical_values = data['static_categorical_values']\n",
    "            self.dynamic_categorical_values = data['dynamic_categorical_values']\n",
    "\n",
    "    def _save_config(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\" Save the preprocessing configuration to a json file.\n",
    "        :param df: input DataFrame\n",
    "        :param filename: filename of the json file without extension.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        data['final_columns'] = df.columns.tolist()\n",
    "        data['static_categorical_values'] = self.static_categorical_values\n",
    "        data['dynamic_categorical_values'] = self.dynamic_categorical_values\n",
    "        with open(filename + '.json', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "\n",
    "    def run_future_activity_training(self, activity_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Runs the preprocessing for the use case of an activity happening in the\n",
    "        future for training data.\n",
    "\n",
    "        :param activity_name: the name of the activity to investigate\n",
    "        :return: DataFrame with the preprocessed training data\n",
    "        \"\"\"\n",
    "        activity_df, case_df = self._processed_activity_case_tables_train()\n",
    "        # Merge activity and case DataFrames\n",
    "        if self.case_table:\n",
    "            joined_df = self._join_dfs([activity_df, case_df], [self.activity_case_key, self.case_case_key])\n",
    "        else:\n",
    "            joined_df = activity_df\n",
    "        # Generate Prefixes\n",
    "        joined_prefixes_df = self._gen_prefixes(joined_df, self.activity_case_key, self.prefixes_case_key,\n",
    "            self.min_prefixes, self.max_prefixes, )\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_label_future_activity(activity_df, joined_prefixes_df, activity_name)\n",
    "        # Aggregate static and dynamic values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_categorical_cols, )\n",
    "\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_numerical_cols, )\n",
    "        aggregate_dynamic_categorical_df = self._aggregate_dynamic_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_categorical_cols,\n",
    "            aggregations=self.aggregations_dyn_cat, )\n",
    "        aggregate_dynamic_numerical_df = self._aggregate_dynamic_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_numerical_cols,\n",
    "            aggregations=self.aggregations_dyn_num, )\n",
    "        aggregate_time_past_df = self._compute_past_time(joined_prefixes_df, self.prefixes_case_key, self.eventtime_col)\n",
    "        # Join the aggregated DataFrames\n",
    "        if self.case_table:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_static_categorical_df, aggregate_static_numerical_df, aggregate_dynamic_categorical_df,\n",
    "                    aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 6, )\n",
    "        else:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_dynamic_categorical_df, aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 4, )\n",
    "        return aggregated_df\n",
    "\n",
    "    def run_remaining_time(self, inference=False) -> pd.DataFrame:\n",
    "        \"\"\"Runs the preprocessing for the use case of predicting the remaining time for\n",
    "        training data.\n",
    "\n",
    "        :return: DataFrame with the preprocessed training data\n",
    "        \"\"\"\n",
    "        # Load config data from json file if inference == True\n",
    "        self.config_file_name = self.dm.name + \"__remaining_time\"\n",
    "        if inference:\n",
    "            self._load_config(self.config_file_name)\n",
    "\n",
    "        activity_df, case_df = self._processed_activity_case_tables_train(inference=inference)\n",
    "\n",
    "        # Merge activity and case DataFrames\n",
    "        if self.case_table:\n",
    "            joined_df = self._join_dfs([activity_df, case_df], [self.activity_case_key, self.case_case_key])\n",
    "        else:\n",
    "            joined_df = activity_df\n",
    "        # Generate Prefixes\n",
    "        joined_prefixes_df = self._gen_prefixes(joined_df, self.activity_case_key, self.prefixes_case_key,\n",
    "            self.min_prefixes, self.max_prefixes, )\n",
    "\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_label_remaining_execution_time(activity_df, joined_prefixes_df)\n",
    "        # Aggregate static and dynamic values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_categorical_cols, )\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_numerical_cols, )\n",
    "\n",
    "        aggregate_dynamic_categorical_df = self._aggregate_dynamic_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_categorical_cols,\n",
    "            aggregations=self.aggregations_dyn_cat, )\n",
    "\n",
    "        aggregate_dynamic_numerical_df = self._aggregate_dynamic_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_numerical_cols,\n",
    "            aggregations=self.aggregations_dyn_num, )\n",
    "        aggregate_time_past_df = self._compute_past_time(joined_prefixes_df, self.prefixes_case_key, self.eventtime_col)\n",
    "        # Join the aggregated DataFrames\n",
    "        if self.case_table:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_static_categorical_df, aggregate_static_numerical_df, aggregate_dynamic_categorical_df,\n",
    "                    aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 6, )\n",
    "        else:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_dynamic_categorical_df, aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 4, )\n",
    "\n",
    "        # Save json file if inference == False\n",
    "        # Add missing columns if inference = True\n",
    "        # if inference:\n",
    "        #    aggregated_df = self._add_missing_columns(aggregated_df, final_columns)\n",
    "\n",
    "        if not inference:\n",
    "            self._save_config(aggregated_df, self.config_file_name)\n",
    "\n",
    "        return aggregated_df\n",
    "\n",
    "    def run_transition_time(self, activity_out: Optional[Sequence[str]], activity_in: Optional[Sequence[str]]):\n",
    "        activity_df, case_df = self._processed_activity_case_tables_train((activity_out, activity_in))\n",
    "\n",
    "        # Merge activity and case DataFrames\n",
    "        if self.case_table:\n",
    "            joined_df = self._join_dfs([activity_df, case_df], [self.activity_case_key, self.case_case_key])\n",
    "        else:\n",
    "            joined_df = activity_df\n",
    "\n",
    "        # Generate Prefixes\n",
    "        joined_prefixes_df = self._gen_prefixes_transitions(joined_df, activity_out, activity_in, )\n",
    "\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_label_transition_times(joined_prefixes_df, activity_out, activity_in)\n",
    "\n",
    "        # Adjust eventtime\n",
    "        joined_prefixes_df = self._adjust_eventtime_transition_times(joined_prefixes_df)\n",
    "\n",
    "        # Aggregate static and dynamic values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_categorical_cols, )\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_numerical_cols, )\n",
    "        aggregate_dynamic_categorical_df = self._aggregate_dynamic_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_categorical_cols,\n",
    "            aggregations=self.aggregations_dyn_cat, )\n",
    "        aggregate_dynamic_numerical_df = self._aggregate_dynamic_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_numerical_cols,\n",
    "            aggregations=self.aggregations_dyn_num, )\n",
    "        aggregate_time_past_df = self._compute_past_time(joined_prefixes_df, self.prefixes_case_key, self.eventtime_col)\n",
    "        # Join the aggregated DataFrames\n",
    "        if self.case_table:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_static_categorical_df, aggregate_static_numerical_df, aggregate_dynamic_categorical_df,\n",
    "                    aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 6, )\n",
    "        else:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_dynamic_categorical_df, aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 4, )\n",
    "        return aggregated_df\n",
    "\n",
    "    def run_decision_point(self, activity_out: str, activity_in: Optional[Sequence[str]]):\n",
    "        activity_out = [activity_out]\n",
    "\n",
    "        activity_df, case_df = self._processed_activity_case_tables_train((activity_out, None))\n",
    "\n",
    "        activity_df = self._add_start_end_activities(activity_df)\n",
    "        # Merge activity and case DataFrames\n",
    "        if self.case_table:\n",
    "            joined_df = self._join_dfs([activity_df, case_df], [self.activity_case_key, self.case_case_key])\n",
    "        else:\n",
    "            joined_df = activity_df\n",
    "\n",
    "        # Generate Prefixes\n",
    "        joined_prefixes_df = self._gen_prefixes_transitions(joined_df, activity_out, None, )\n",
    "\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_label_next_activity(joined_prefixes_df, activity_in)\n",
    "\n",
    "        # Remove last activities from the joined DataFrame such that no future\n",
    "        # information is used.\n",
    "        joined_prefixes_df = self._remove_last_row(joined_prefixes_df)\n",
    "\n",
    "        # Aggregate static and dynamic values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_categorical_cols, )\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.static_numerical_cols, )\n",
    "        aggregate_dynamic_categorical_df = self._aggregate_dynamic_categorical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_categorical_cols,\n",
    "            aggregations=self.aggregations_dyn_cat, )\n",
    "        aggregate_dynamic_numerical_df = self._aggregate_dynamic_numerical(joined_prefixes_df,\n",
    "            case_key=self.prefixes_case_key, columns=self.dynamic_numerical_cols,\n",
    "            aggregations=self.aggregations_dyn_num, )\n",
    "        aggregate_time_past_df = self._compute_past_time(joined_prefixes_df, self.prefixes_case_key, self.eventtime_col)\n",
    "        # Join the aggregated DataFrames\n",
    "        if self.case_table:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_static_categorical_df, aggregate_static_numerical_df, aggregate_dynamic_categorical_df,\n",
    "                    aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 6, )\n",
    "        else:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_dynamic_categorical_df, aggregate_dynamic_numerical_df, aggregate_time_past_df, labels_df, ],\n",
    "                [self.prefixes_case_key] * 4, )\n",
    "        return aggregated_df\n",
    "\n",
    "    def run_total_time_training(self) -> pd.DataFrame:\n",
    "        \"\"\"Runs the preprocessing for the use case of explaining the total trace time for\n",
    "        training data.\n",
    "\n",
    "        :return: DataFrame with the preprocessed training data\n",
    "        \"\"\"\n",
    "        activity_df, case_df = self._processed_activity_case_tables_train()\n",
    "        # Merge activity and case DataFrames\n",
    "        if self.case_table:\n",
    "            joined_df = self._join_dfs([activity_df, case_df], [self.activity_case_key, self.case_case_key])\n",
    "        else:\n",
    "            joined_df = activity_df\n",
    "\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_label_total_time(joined_df, self.activity_case_key)\n",
    "\n",
    "        # Aggregate static and dynamic values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(joined_df, case_key=self.activity_case_key,\n",
    "            columns=self.static_categorical_cols, )\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(joined_df, case_key=self.activity_case_key,\n",
    "            columns=self.static_numerical_cols, )\n",
    "        aggregate_dynamic_categorical_df = self._aggregate_dynamic_categorical(joined_df,\n",
    "            case_key=self.activity_case_key, columns=self.dynamic_categorical_cols,\n",
    "            aggregations=self.aggregations_dyn_cat, )\n",
    "        aggregate_dynamic_numerical_df = self._aggregate_dynamic_numerical(joined_df, case_key=self.activity_case_key,\n",
    "            columns=self.dynamic_numerical_cols, aggregations=self.aggregations_dyn_num, )\n",
    "\n",
    "        # Join the aggregated DataFrames\n",
    "        if self.case_table:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_static_categorical_df, aggregate_static_numerical_df, aggregate_dynamic_categorical_df,\n",
    "                    aggregate_dynamic_numerical_df, labels_df, ], [self.activity_case_key] * 5, )\n",
    "        else:\n",
    "            aggregated_df = self._join_dfs(\n",
    "                [aggregate_dynamic_categorical_df, aggregate_dynamic_numerical_df, labels_df, ],\n",
    "                [self.activity_case_key] * 3, )\n",
    "        return aggregated_df\n",
    "\n",
    "    def run_deviations(self, analysis, shared_url) -> pd.DataFrame:\n",
    "        \"\"\"Runs the preprocessing for the use case of explaining the total trace time for\n",
    "        training data.\n",
    "\n",
    "        :return: DataFrame with the preprocessed training data\n",
    "        \"\"\"\n",
    "        case_df = self._processed_case_table_train()\n",
    "\n",
    "        # Get case keys of conforming cases\n",
    "        case_keys_conforming = self._query_case_keys_conforming(analysis, shared_url)\n",
    "\n",
    "        # Generate labels\n",
    "        labels_df = self._gen_labels_conforming(case_df, self.case_case_key, case_keys_conforming)\n",
    "\n",
    "        # Aggregate static values\n",
    "        aggregate_static_categorical_df = self._aggregate_static_categorical(case_df, case_key=self.case_case_key,\n",
    "            columns=self.static_categorical_cols, )\n",
    "\n",
    "        aggregate_static_numerical_df = self._aggregate_static_numerical(case_df, case_key=self.case_case_key,\n",
    "            columns=self.static_numerical_cols, )\n",
    "\n",
    "        # Join the aggregated DataFrames\n",
    "        aggregated_df = self._join_dfs([aggregate_static_categorical_df, aggregate_static_numerical_df, labels_df, ],\n",
    "            [self.case_case_key] * 3, )\n",
    "\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}